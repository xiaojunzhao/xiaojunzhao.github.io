---
layout: post
title: Understanding of Long Short Term Memory Neural Network and Python Implementation
permalink: blog/LSTM/
---

<p style='text-align: justify;'>LSTMs, short for Long Short Term Memory networks, is a variant of Recurrent Neural Networks and introduced to solve the vanishing gradient problem of RNN when we need to backpropagate error across many timestamps, in other words, it is capable of learning long-term dependencies. This blog will quickly go over the structure of LSTM and how it works.we will focus on deriving gradients step by step in details.At last, we'll implement a simple LSTM model using pure python code. </p>

<p style='text-align: justify;'>Firstly, let's understand the architecture of LSTM and how feedforward pass works. The picture below is from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah's blog</a> </p>
<center>
<img src="/images/lstm_structure.png" class="center" style="width: 70%; height: 70%" >
</center>
<p style='text-align: justify;'>Information passes through three gates in each of the LSTM node across timestamps.</p>

1. Forget Gate: 
<center>
<img src="/images/forget.png" class="center" style="width: 70%; height: 70%" >
</center>

<p style='text-align: justify;'>The forget gate \(f(t)\) is capable of deciding which information to forget from the cell state \(C(t)\)</p>
