---
layout: post
title: Understanding of Long Short Term Memory Neural Network and Its Python Implementation
permalink: blog/LSTM/
---

<p style='text-align: justify;'>LSTMs, short for Long Short Term Memory networks, is a variant of Recurrent Neural Networks and introduced to solve the vanishing gradient problem of RNN when we need to backpropagate error across many timestamps, in other words, it is capable of learning long-term dependencies. This blog will quickly go over the structure of LSTM and how it works.we will focus on deriving gradients step by step in details.At last, we'll implement a simple LSTM model from scratch using pure python and numpy. </p>

<p style='text-align: justify;'>Firstly, let's understand the architecture of LSTM and how feedforward pass works. The picture below is from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah's blog</a> </p>
<center>
<img src="/images/lstm_structure.png" class="center" style="width: 70%; height: 70%" >
</center>
<p style='text-align: justify;'>Information passes through three gates in each of the LSTM node across timestamps.</p>

* Forget Gate: 
<center>
<img src="/images/forget.png" class="center" style="width: 70%; height: 70%" >
</center>

<p style='text-align: justify;'>The forget gate \(f_t\) is capable of deciding which information to forget from the previous cell state \(C_{t-1}\), because the output of the sigmoid function \(\sigma\) is a single value between 0 and 1. Multiplying this value with an element of \(C_{t-1}\) means how much information we're going to throw away, or put in another way, how much informatio we're going to keep.  Here, we notice that we concate the previous hidden state \(h_{t-1}\) with the input vector \(X_{t}\) together. Let's assume the input \(X_{t}\) is a \(D \times 1\) vector, and \(h_{t-1}\) has \(H\) neurons, then the concated vector \([h_{t-1},X_{t}]\) is a \((H+D) \times 1\) vector. </p>

* Input Gate: 
<center>
<img src="/images/input.png" class="center" style="width: 70%; height: 70%" >
</center>

<p style='text-align: justify;'> The input gate involves two parts, the \(sigmoid\) layer decides which of the new information should be updated or ignored, and \(tanh\) layer creates a vector of new candidate values. Those two outputs are multiplied to update the cell state \(C_{t}\) as shown below:  </p>

<center>
<img src="/images/input1.png" class="center" style="width: 70%; height: 70%" >
</center>

* Output Gate:
<center>
<img src="/images/output.png" class="center" style="width: 70%; height: 70%" >
</center>

<p style='text-align: justify;'> In output gate, we will output hidden state \(h_{t}\) which depends on the previous hidden state \(h_{t-1}\) and current Cell state \(C_t\). Then we softmax the hidden state \(h_t\) multiplying the weights connecting lstm node with the nn output layer as predictions. The loss function is cross entropy for classification problems, and mean square error for regression problems. $$E_t = -\sum_{i=1}^{kclasses}\y_ilog\hat{y_i} $$</p>









